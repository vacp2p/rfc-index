# CODEX-DHT

| Field | Value |
| --- | --- |
| Name | Codex Discovery |
| Slug | |
| Status | raw |
| Category | Standards Track |
| Editor | Jimmy Debe <jimmy@status.im> |
| Contributors | Filip Dimitrijevic <filip@status.im> |

## Abstract

This document specifies the Codex DHT (Distributed Hash Table) component,
a modified version of the DiscV5 protocol used to store Codex signed peer records (SPR)
and content identifiers (CID) for each host.
The DHT enables peer discovery and content routing in the Codex network.

**Keywords:** DHT, distributed hash table, DiscV5, peer discovery, SPR,
signed peer record, routing table, Kademlia

## Semantics

The keywords "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT",
"SHOULD", "SHOULD NOT", "RECOMMENDED", "MAY", and "OPTIONAL"
in this document are to be interpreted as described in [RFC 2119][rfc-2119].

### Definitions

| Term | Description |
| ---- | ----------- |
| Provider | A node running the Codex protocol and providing resources to the Codex network. |
| NodeId | A unique identifier for a node, generated as the keccak256 hash of the node's public key. |
| SPR | Signed Peer Record, containing a provider's connection information signed with their private key. |
| CID | Content Identifier, a self-describing content-addressed identifier for data stored on the network. |
| Routing Table | A local data structure storing SPRs of discovered providers, organized into k-buckets. |
| K-Bucket | A subdivision of the routing table covering a range of NodeIds, sorted by last seen time. |

## Background

Codex is a network of nodes, identified as providers,
participating in a decentralized peer-to-peer storage protocol.
The decentralized storage solution offers data durability guarantees,
incentive mechanisms, and data persistence guarantees.

The Codex DHT component is a modified version of the
[DiscV5 DHT][discv5] protocol to store Codex SPR entries
and the CID for each host.
DiscV5 is a node discovery system used to find peers who are registered on a distributed hash table (DHT).
This allows a provider to publish to the network their connection information and
information about what content they are storing.
A Codex provider will support this protocol at no extra cost other than the use of resources to store node records.
This allows any provider node to be used as the entry point for new providers to connect to live nodes on the Codex network.

## Protocol Specification

### Provider Registration

A `provider` is a node running the Codex protocol and providing resources to the Codex network.
To become a `provider`, the node MUST have a `NodeId`,
which is generated with the keccak256 hash function of its `PublicKey`.
The record will be shared and
stored by other `provider`s in their local routing table.
A `FINDNODEMESSAGE` message is used by the new `provider` to query other nodes who may choose to store the new record.
Once stored by one `provider`,
the new `provider` will be accessible to the network.

This record SHOULD include node identity information, connection information,
timing information, and reliability information.
Information provided in the record can be updated at any time to match the live details of the `provider`.
The following is the `provider` node record in the Codex network.

```js
{
   "Provider" : {
        id: NodeId
        pubkey: PublicKey
        address: Array[peerId]
        record: SignedPeerRecord
        seen: float // Indicates if there was at least one successful
        // request-response with this provider, or if the node was verified
        // through the underlying transport mechanisms. After first contact
        // it tracks how reliable is the communication with the provider.
        stats: Stats // traffic measurements and statistics
    }

    "SignedPeerRecord" : {
      "seqNum": uint64, // sequence number of record update
      "pubkey": PublicKey,
      "ip": IpAddress, // ip address is optional
      "tcpPort": Port,
      "udpPort": Port
    }

   "NodeId" : keccak256(PublicKey)

   "Stats" : {
       "rttMin": float // millisec
       "rttAvg": float // millisec
       "bwAvg": float // bps
       "bwMax": float // bps
   }
}
```

As in the DiscV5 protocol, the `NodeId` is a unique identifier to find other nodes within the network.
The `peerId` and `NodeId` MUST NOT be the same.

### Signed Peer Record

The `record` MUST be generated by the `provider`,
which contains their connection information.
On the Codex network,
the `record` is identified as a `SignedPeerRecord` (SPR).

All values, excluding the `ip`, are REQUIRED in a SPR.
Which nodes and the number of nodes in this set are described in the [Routing Table](#routing-table) section.
The `PrivateKey` MUST be used to sign the `record`.
A `provider` SHOULD disregard messages from a node if the `record` is unsigned or becomes stale.
The `provider` SHOULD contact other live nodes to disseminate new and updated records.
The update will increase the `seqNum`, then sign the new version of the `record`.

### Distance Calculation

The distance between two nodes is calculated using the XOR metric on their NodeIds,
as specified in the [DiscV5 specification][discv5].

### Routing Table

Each `provider` has a local routing table which stores the SPR of other `provider`s it has discovered.
New `provider`s SHOULD query live nodes to update their local routing table with SPRs.

```js
{
   "RoutingTable" : {
       "localProvider": Provider,
       "buckets": seq[KBucket],
       "bitsPerHop": number,
       "ipLimits": IpLimits, // IP limits for total routing table: all buckets and
       // replacement caches.
       "distanceCalculator" : DistanceCalculator,
       "rng" : ref HmacDrbgContext
   }

   "KBucket" : {
       "istart", "iend": NodeId,
       "providers": seq[Provider],
       "replacementCache": seq[Providers], // Nodes that could not be added to the `providers`
       // seq as it is full and without stale nodes. This is practically a small
       // LRU cache.
       "ipLimits": IpLimits, // IP limits for bucket: node entries and replacement
       // cache entries combined.
   }

   "IpLimits" : {
      "tableIpLimit": number,
      "bucketIpLimit": number
   }
}
```

The `bitsPerHop` MUST indicate the minimum number of bits of a `NodeId` needed to get closer to finding the target per query.
Practically, it tells a `provider` also how often a node "not in range", based on `NodeId` prefix similarities,
will cause a branch to split off.
Setting this value to 1 is the basic, non-accelerated version,
which will never split off the "not in range" branch and
which will result in $\log_2 n$ hops per lookup.
Setting it higher will increase the amount of splitting on a "not in range" branch,
thus holding more `providers` with a better keyspace coverage and
will result in an improvement of $\log_{2^b} n$ hops per lookup.

- `DistanceCalculator`: value SHOULD be generated with the defined node distance algorithm used in the [DiscV5 specification][discv5].
- `istart`: The range of `NodeId`s this `KBucket` covers.
  This is not a simple logarithmic distance, as buckets can be split over a prefix that
  does not cover the `localNode` id.
- `providers`: Node entries of the KBucket are sorted according to last time seen.
  First entry (head) is considered the most recently seen node, and
  the last entry (tail) is considered the least recently seen node.
  Here, "seen" indicates a successful request-response.
  This can also not have occurred yet.
- `IpLimits`: The routing table IP limits are applied on both the total table,
  and on the individual buckets.
  In each case, the active node entries,
  but also the entries waiting in the replacement cache, are accounted for.
  This way, the replacement cache cannot get filled with nodes that then cannot be added due to the limits that apply.
  As entries are not verified immediately before or on entry,
  a malicious node MAY fill the routing table or
  a specific bucket with SPRs that have `ip`s not controlled by that adversary.
  This would affect the `provider` that actually controls the `ip`,
  as they could have a difficult time disseminating its SPR to be stored in the DHT by other `provider`s.
  However, that `provider` can still search and find nodes to connect to.

There is the possibility to set the `IPLimits` on verified `providers` only,
but that would allow for lookups to be done on a higher set of nodes owned by the same identity.
This is a worse alternative.
Doing lookups only on verified nodes would slow down discovery startup.

## Security Considerations

### Record Authenticity

All SPRs MUST be signed with the provider's private key.
Implementations MUST verify the signature before accepting any SPR.
Unsigned or improperly signed records SHOULD be rejected.

### IP Limit Protection

The routing table implements IP limits to prevent Sybil attacks
where an adversary attempts to fill the routing table with malicious entries.
Implementations SHOULD enforce both per-bucket and total table IP limits
to mitigate this attack vector.

### Stale Record Handling

Providers SHOULD discard records that have become stale or outdated.
The `seqNum` field provides version information to determine record freshness.
Implementations SHOULD prefer records with higher sequence numbers.

## References

### Normative

- [DiscV5 DHT][discv5] - Ethereum Node Discovery Protocol v5

### Informative

- [Ethereum Node Record][enr] - ENR specification
- [Component Specification - Discovery][origin-ref] - Original specification

[rfc-2119]: https://www.ietf.org/rfc/rfc2119.txt
[discv5]: https://github.com/ethereum/devp2p/blob/master/discv5/discv5.md
[enr]: https://github.com/ethereum/devp2p/blob/master/enr.md
[origin-ref]: https://github.com/logos-storage/logos-storage-docs-obsidian/blob/main/10%20Notes/Specs/Component%20Specification%20-%20Discovery.md

## Copyright

Copyright and related rights waived via [CC0](https://creativecommons.org/publicdomain/zero/1.0/).
